{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Built-in logging lab\n",
    "![flow](../../images/built-in-logging.gif)\n",
    "\n",
    "Playground to try routing to a backend based on Azure OpenAI model and version.\n",
    "\n",
    "### TOC\n",
    "- [0Ô∏è‚É£ Initialize notebook variables](#0)\n",
    "- [1Ô∏è‚É£ Create the Azure Resource Group](#1)\n",
    "- [2Ô∏è‚É£ Create deployment using ü¶æ Bicep](#2)\n",
    "- [3Ô∏è‚É£ Get the deployment outputs](#3)\n",
    "- [üß™ Test the API using a direct HTTP call](#requests)\n",
    "- [üß™ Test the API using the Azure OpenAI Python SDK](#sdk)\n",
    "- [üóëÔ∏è Clean up resources](#clean)\n",
    "\n",
    "### Backlog\n",
    "- Improve the notebook\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.8 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/en-us/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-interactively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id\n",
    "- The ```mock_webapps``` variable sets the list of deployed Web Apps for the mocking functionality. Clean the ```openai_resources``` list to simulate the OpenAI behaviour with the mocking service.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "index = '2'\n",
    "resource_group_name = f\"lab-{deployment_name}{index}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "\n",
    "# Define two OpenAI model and version combinations\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35\n",
    "# Please note that availability of models and versions is variable and that you may need to adjust the model and version names to match the available models and versions in your Azure subscription.\n",
    "# For this lab, we are using the following combinations based on PayGo availability on June 20, 2024:\n",
    "#\n",
    "#   1) GPT-3.5 Turbo 1106: France Central, Sweden Central\n",
    "#   2) GPT-3.5 Turbo 0125: North Central US, South Central US\n",
    "\n",
    "openai_model_1_name = \"gpt-35-turbo\"\n",
    "openai_model_1_version = \"1106\"\n",
    "openai_deployment_1_name = f\"{openai_model_1_name}-{openai_model_1_version}\"\n",
    "openai_resources_1 = [ {\"name\": \"oai-francecentral\", \"location\": \"francecentral\"}, {\"name\": \"oai-swedencentral\", \"location\": \"swedencentral\"} ] # list of OpenAI resources to deploy. Clear this list to use only the mock resources\n",
    "\n",
    "openai_model_2_name = \"gpt-35-turbo\"\n",
    "openai_model_2_version = \"0125\"\n",
    "openai_deployment_2_name = f\"{openai_model_2_name}-{openai_model_2_version}\"\n",
    "openai_resources_2 = [ {\"name\": \"oai-northcentralus\", \"location\": \"northcentralus\"}, {\"name\": \"oai-southcentralus\", \"location\": \"southcentralus\"} ] # list of OpenAI resources to deploy. Clear this list to use only the mock resources\n",
    "\n",
    "# Define Azure OpenAI resources\n",
    "openai_resources_sku = \"S0\"\n",
    "openai_api_version = \"2024-02-01\"\n",
    "openai_specification_url='https://raw.githubusercontent.com/Azure/azure-rest-api-specs/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/' + openai_api_version + '/inference.json'\n",
    "\n",
    "# Define Azure API Management\n",
    "apim_resource_name = \"apim\"\n",
    "apim_resource_location = \"westeurope\"\n",
    "apim_resource_sku = \"Basicv2\"\n",
    "\n",
    "# Define the Azure OpenAI backends and backend pools per Azure OpenAI model and version\n",
    "openai_backend_pool_1 = f\"oai-backend-pool-{openai_deployment_1_name}\"\n",
    "openai_backend_pool_2 = f\"oai-backend-pool-{openai_deployment_2_name}\"\n",
    "\n",
    "log_analytics_name = \"workspace\"\n",
    "app_insights_name = 'insights'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Create the Azure Resource Group\n",
    "All resources deployed in this lab will be created in the specified resource group. Skip this step if you want to use an existing resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure Resource Group  lab-model-routing1  created ‚åö  16:15:51.170076\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "resource_group_stdout = ! az group create --name {resource_group_name} --location {resource_group_location} \n",
    "\n",
    "if resource_group_stdout.n.startswith(\"ERROR\"):\n",
    "    print(resource_group_stdout)\n",
    "else:\n",
    "    print(\"‚úÖ Azure Resource Group \", resource_group_name, \" created ‚åö \", datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This lab uses [Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview?tabs=bicep) to declarative define all the resources that will be deployed. Change the parameters or the [main.bicep](main.bicep) directly to try different configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: {\"code\": \"ResourceGroupBeingDeleted\", \"message\": \"The resource group 'lab-model-routing' is in deprovisioning state and cannot perform this operation.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(openai_resources_1) > 0:\n",
    "    backend_id_1 = openai_backend_pool_1 if len(openai_resources_1) > 1 else openai_resources_1[0].get(\"name\")\n",
    "if len(openai_resources_2) > 0:\n",
    "    backend_id_2 = openai_backend_pool_2 if len(openai_resources_2) > 1 else openai_resources_2[0].get(\"name\")\n",
    "\n",
    "with open(\"policy.xml\", 'r') as policy_xml_file:\n",
    "    policy_template_xml = policy_xml_file.read()\n",
    "    policy_xml = policy_template_xml.replace(\"{backend-id-1}\", backend_id_1).replace(\"{backend-id-2}\", backend_id_2)\n",
    "    policy_xml_file.close()\n",
    "open(\"policy.xml\", 'w').write(policy_xml)\n",
    "\n",
    "bicep_parameters = {\n",
    "  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n",
    "  \"contentVersion\": \"1.0.0.0\",\n",
    "  \"parameters\": {\n",
    "    \"openAIBackendPoolName_1\": { \"value\": openai_backend_pool_1 },\n",
    "    \"openAIBackendPoolName_2\": { \"value\": openai_backend_pool_2 },\n",
    "    \"openAIConfig_1\": { \"value\": openai_resources_1 },\n",
    "    \"openAIConfig_2\": { \"value\": openai_resources_2 },\n",
    "    \"openAIDeploymentName_1\": { \"value\": openai_deployment_1_name },\n",
    "    \"openAIDeploymentName_2\": { \"value\": openai_deployment_2_name },\n",
    "    \"openAISku\": { \"value\": openai_resources_sku },\n",
    "    \"openAIModelName_1\": { \"value\": openai_model_1_name },\n",
    "    \"openAIModelName_2\": { \"value\": openai_model_2_name },\n",
    "    \"openAIModelVersion_1\": { \"value\": openai_model_1_version },\n",
    "    \"openAIModelVersion_2\": { \"value\": openai_model_2_version },\n",
    "    \"openAIAPISpecURL\": { \"value\": openai_specification_url },\n",
    "    \"apimResourceName\": { \"value\": apim_resource_name},\n",
    "    \"apimResourceLocation\": { \"value\": apim_resource_location},\n",
    "    \"apimSku\": { \"value\": apim_resource_sku},\n",
    "    \"logAnalyticsName\": { \"value\": log_analytics_name },\n",
    "    \"applicationInsightsName\": { \"value\": app_insights_name },\n",
    "    \"index\": { \"value\": index}\n",
    "  }\n",
    "}\n",
    "with open('params.json', 'w') as bicep_parameters_file:\n",
    "    bicep_parameters_file.write(json.dumps(bicep_parameters))\n",
    "\n",
    "! az deployment group create --name {deployment_name} --resource-group {resource_group_name} --template-file \"main.bicep\" --parameters \"params.json\"\n",
    "\n",
    "open(\"policy.xml\", 'w').write(policy_template_xml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "üëâüèª API Gateway URL:  \n",
      "üëâüèª Workspace ID:  ERROR: (DeploymentNotFound) Deployment 'model-routing' could not be found.\n",
      "Code: DeploymentNotFound\n",
      "Message: Deployment 'model-routing' could not be found.\n",
      "üëâüèª App ID:  ERROR: (DeploymentNotFound) Deployment 'model-routing' could not be found.\n",
      "Code: DeploymentNotFound\n",
      "Message: Deployment 'model-routing' could not be found.\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimSubscriptionKey.value -o tsv\n",
    "apim_subscription_key = deployment_stdout.n\n",
    "\n",
    "# type: ignore\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.apimResourceGatewayURL.value -o tsv\n",
    "apim_resource_gateway_url = deployment_stdout.n\n",
    "print(\"üëâüèª API Gateway URL: \", apim_resource_gateway_url)\n",
    "\n",
    "# type: ignore\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.logAnalyticsWorkspaceId.value -o tsv\n",
    "workspace_id = deployment_stdout.n\n",
    "print(\"üëâüèª Workspace ID: \", workspace_id)\n",
    "\n",
    "# type: ignore\n",
    "deployment_stdout = ! az deployment group show --name {deployment_name} -g {resource_group_name} --query properties.outputs.applicationInsightsAppId.value -o tsv\n",
    "app_id = deployment_stdout.n\n",
    "print(\"üëâüèª App ID: \", app_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Run:  1\n",
      "status code:  200\n",
      "headers  {'Content-Length': '930', 'Content-Type': 'application/json', 'Date': 'Thu, 20 Jun 2024 19:10:24 GMT', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache, must-revalidate', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '266cad86-b42f-44d8-99c5-04940e89eee0', 'X-Content-Type-Options': 'nosniff', 'x-ms-region': 'France Central', 'x-ratelimit-remaining-requests': '19', 'x-ratelimit-remaining-tokens': '18680', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '141a9030-dfa7-404d-845e-fcc3c45ba082', 'x-ms-client-request-id': 'Not-Set', 'azureml-model-session': 'd027-20240603054521', 'Request-Context': 'appId=cid-v1:c844dffc-e3d9-471d-828a-78852087a74d'}\n",
      "x-ms-region:  France Central\n",
      "response:  Oh, I'm so sorry, I don't have the capability to tell time. You'll have to consult that fancy, outdated device on your wrist called a watch.\n",
      "‚ñ∂Ô∏è Run:  2\n",
      "status code:  200\n",
      "headers  {'Content-Length': '933', 'Content-Type': 'application/json', 'Date': 'Thu, 20 Jun 2024 19:10:26 GMT', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache, must-revalidate', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '3b38b5ae-f986-4231-9669-4207e6ca402c', 'X-Content-Type-Options': 'nosniff', 'x-ms-region': 'Sweden Central', 'x-ratelimit-remaining-requests': '19', 'x-ratelimit-remaining-tokens': '18680', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '87bee083-04bf-43cc-9453-859cf52f8145', 'x-ms-client-request-id': 'Not-Set', 'azureml-model-session': 'd098-20240606135852', 'Request-Context': 'appId=cid-v1:c844dffc-e3d9-471d-828a-78852087a74d'}\n",
      "x-ms-region:  Sweden Central\n",
      "response:  Sure, why don't you use that funny little device on your wrist or the clock on your phone? That usually works pretty well for telling the time.\n",
      "‚ñ∂Ô∏è Run:  1\n",
      "status code:  200\n",
      "headers  {'Content-Length': '1008', 'Content-Type': 'application/json', 'Date': 'Thu, 20 Jun 2024 19:10:30 GMT', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache, must-revalidate', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '79a83c08-f4a7-43b3-9c77-0c216b2ba23b', 'X-Content-Type-Options': 'nosniff', 'x-ms-region': 'North Central US', 'x-ratelimit-remaining-requests': '19', 'x-ratelimit-remaining-tokens': '19340', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '95b9d90e-a34c-4dfd-83b7-6a849b501ec6', 'azureml-model-session': 'd056-20240601100959', 'x-envoy-upstream-service-time': '1642', 'x-ms-client-request-id': 'Not-Set', 'Request-Context': 'appId=cid-v1:c844dffc-e3d9-471d-828a-78852087a74d'}\n",
      "x-ms-region:  North Central US\n",
      "response:  Oh sure, let me just use my magical clock-reading powers to know exactly what time it is without any additional information. Oh wait, I can't do that. Maybe try looking at an actual clock or your phone. Just a thought.\n",
      "‚ñ∂Ô∏è Run:  2\n",
      "status code:  200\n",
      "headers  {'Content-Length': '1005', 'Content-Type': 'application/json', 'Date': 'Thu, 20 Jun 2024 19:10:32 GMT', 'Access-Control-Allow-Origin': '*', 'Cache-Control': 'no-cache, must-revalidate', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '11cb4d3e-4250-427e-87e7-c7ca36b220a4', 'X-Content-Type-Options': 'nosniff', 'x-ms-region': 'South Central US', 'x-ratelimit-remaining-requests': '19', 'x-ratelimit-remaining-tokens': '19340', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': 'fde074c2-ee1b-4924-b9ae-c3dc867b667f', 'azureml-model-session': 'd048-20240605101034', 'x-envoy-upstream-service-time': '600', 'x-ms-client-request-id': 'Not-Set', 'Request-Context': 'appId=cid-v1:c844dffc-e3d9-471d-828a-78852087a74d'}\n",
      "x-ms-region:  South Central US\n",
      "response:  Oh, sure, let me get my magical clock that can magically transmit the time through the internet to you. Oh, wait, I don't have one of those. Maybe you can try checking the clock on your device or ask someone nearby?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "runs = 2\n",
    "sleep_time_ms = 1000\n",
    "\n",
    "# GPT-3.5 Turbo 1106\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è GPT-3.5 Turbo 1106 Run: \", i+1)\n",
    "    if len(openai_resources_1) > 0:\n",
    "        messages={\"messages\":[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]}\n",
    "\n",
    "    url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_1_name + \"/chat/completions?api-version=\" + openai_api_version        \n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    print(\"status code: \", response.status_code)\n",
    "    print(\"headers \", response.headers)\n",
    "    print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"response: \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    else:\n",
    "        print(response.text)\n",
    "    time.sleep(sleep_time_ms/1000)\n",
    "\n",
    "# GPT-3.5 Turbo 0125\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è GPT-3.5 Turbo 0125 Run: \", i+1)\n",
    "    if len(openai_resources_1) > 0:\n",
    "        messages={\"messages\":[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]}\n",
    "\n",
    "    url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_2_name + \"/chat/completions?api-version=\" + openai_api_version        \n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    print(\"status code: \", response.status_code)\n",
    "    print(\"headers \", response.headers)\n",
    "    print(\"x-ms-region: \", response.headers.get(\"x-ms-region\")) # this header is useful to determine the region of the backend that served the request\n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"response: \", data.get(\"choices\")[0].get(\"message\").get(\"content\"))\n",
    "    else:\n",
    "        print(response.text)\n",
    "    time.sleep(sleep_time_ms/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "OpenAPI provides a widely used [Python library](https://github.com/openai/openai-python). The library includes type definitions for all request params and response fields. The goal of this test is to assert that APIM can seamlessly proxy requests to OpenAI without disrupting its functionality.\n",
    "- Note: run ```pip install openai``` in a terminal before executing this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è GPT-3.5 Turbo 1106 Run:  1\n",
      "Sure, let me just consult my watch that doesn't exist.\n",
      "‚ñ∂Ô∏è GPT-3.5 Turbo 1106 Run:  2\n",
      "Sure, let me just conjure a clock out of thin air and tell you the time. Oh wait, I can't do that. Why don't you look at a clock or your phone like a normal person?\n",
      "‚ñ∂Ô∏è GPT-3.5 Turbo 0125 Run:  1\n",
      "Sure, have you considered looking at a clock or your phone? It tends to have that information readily available.\n",
      "‚ñ∂Ô∏è GPT-3.5 Turbo 0125 Run:  2\n",
      "Oh, I‚Äôm so glad you asked me that! *eye roll* Did you know that there are devices specifically designed for determining the time? It‚Äôs a whole new world out there!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "runs = 2\n",
    "sleep_time_ms = 1000\n",
    "\n",
    "# GPT-3.5 Turbo 1106\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è GPT-3.5 Turbo 1106 Run: \", i+1)\n",
    "    from openai import AzureOpenAI\n",
    "    if len(openai_resources_1) > 0:\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]\n",
    "        \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    response = client.chat.completions.create(model=openai_deployment_1_name, messages=messages)\n",
    "    print(response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)\n",
    "\n",
    "# GPT-3.5 Turbo 0125\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è GPT-3.5 Turbo 0125 Run: \", i+1)\n",
    "    from openai import AzureOpenAI\n",
    "    if len(openai_resources_2) > 0:\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ]\n",
    "        \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "    response = client.chat.completions.create(model=openai_deployment_2_name, messages=messages)\n",
    "    print(response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kql'></a>\n",
    "### üîç Analyze Application Insights requests\n",
    "\n",
    "With this query you can get the request and response details including the prompt and the OpenAI completion. It also returns token counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 27\u001b[0m\n\u001b[0;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests  \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m| project timestamp, duration, customDimensions \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m| extend duration = round(duration, 2) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m| project timestamp, apiName, apimSubscription, duration, userAgent, model, messages, completion, region, promptTokens, completionTokens, totalTokens, remainingTokens, remainingRequests \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m| order by timestamp desc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m result_stdout \u001b[38;5;241m=\u001b[39m get_ipython()\u001b[38;5;241m.\u001b[39mgetoutput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m az monitor app-insights query --app \u001b[39m\u001b[38;5;132;01m{app_id}\u001b[39;00m\u001b[38;5;124m --analytics-query \u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_stdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m table \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m), columns\u001b[38;5;241m=\u001b[39m[col\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\simonkurtz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\simonkurtz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\simonkurtz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"\\\"\" + \"requests  \\\n",
    "| project timestamp, duration, customDimensions \\\n",
    "| extend duration = round(duration, 2) \\\n",
    "| extend parsedCustomDimensions = parse_json(customDimensions) \\\n",
    "| extend apiName = tostring(parsedCustomDimensions.['API Name']) \\\n",
    "| extend apimSubscription = tostring(parsedCustomDimensions.['Subscription Name']) \\\n",
    "| extend userAgent = tostring(parsedCustomDimensions.['Request-User-agent']) \\\n",
    "| extend request_json = tostring(parsedCustomDimensions.['Request-Body']) \\\n",
    "| extend request = parse_json(request_json) \\\n",
    "| extend model = tostring(request.['model']) \\\n",
    "| extend messages = tostring(request.['messages']) \\\n",
    "| extend region = tostring(parsedCustomDimensions.['Response-x-ms-region']) \\\n",
    "| extend remainingTokens = tostring(parsedCustomDimensions.['Response-x-ratelimit-remaining-tokens']) \\\n",
    "| extend remainingRequests = tostring(parsedCustomDimensions.['Response-x-ratelimit-remaining-requests']) \\\n",
    "| extend response_json = tostring(parsedCustomDimensions.['Response-Body']) \\\n",
    "| extend response = parse_json(response_json) \\\n",
    "| extend promptTokens = tostring(response.['usage'].['prompt_tokens']) \\\n",
    "| extend completionTokens = tostring(response.['usage'].['completion_tokens']) \\\n",
    "| extend totalTokens = tostring(response.['usage'].['total_tokens']) \\\n",
    "| extend completion = tostring(response.['choices'][0].['message'].['content']) \\\n",
    "| project timestamp, apiName, apimSubscription, duration, userAgent, model, messages, completion, region, promptTokens, completionTokens, totalTokens, remainingTokens, remainingRequests \\\n",
    "| order by timestamp desc\" + \"\\\"\"\n",
    "\n",
    "result_stdout = ! az monitor app-insights query --app {app_id} --analytics-query {query} # type: ignore\n",
    "result = json.loads(result_stdout.n)\n",
    "\n",
    "table = result.get('tables')[0]\n",
    "pd.DataFrame(table.get(\"rows\"), columns=[col.get(\"name\") for col in table.get('columns')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='portal'></a>\n",
    "### üîç Open the workbook in the Azure Portal\n",
    "\n",
    "Go to the application insights resource and under the Monitoring section select the Workbooks blade. You should see the OpenAI Usage Analysis workbook with the above query and some others to check token counts, performance, failures, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
